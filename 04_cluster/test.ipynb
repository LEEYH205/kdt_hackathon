{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 더미 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    idea_id        title                                            body  좋아요  \\\n",
      "0  idea_001   로봇 바리스타 카페  무인 로봇 팔이 주문과 커피 제조를 담당해 24시간 운영하는 스마트 카페 아이디어.   39   \n",
      "1  idea_002  반려동물 1인 미용실      셀프 그루밍 부스와 전문가 예약 서비스를 결합한 동네 소형 반려동물 미용실.   16   \n",
      "\n",
      "   싫어요                                               text  \n",
      "0    0  로봇 바리스타 카페 무인 로봇 팔이 주문과 커피 제조를 담당해 24시간 운영하는 스...  \n",
      "1   15  반려동물 1인 미용실 셀프 그루밍 부스와 전문가 예약 서비스를 결합한 동네 소형 반...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 예시 CSV (idea_id,title,body)\n",
    "df = pd.read_csv(\"./data/ideas_sample_1000.csv\").fillna(\"\")\n",
    "df[\"text\"] = df[\"title\"].str.strip() + \" \" + df[\"body\"].str.strip()\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 전처리(클렌징)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean(txt: str) -> str:\n",
    "    txt = re.sub(r\"http\\S+|www\\S+\", \" \", txt)            # URL\n",
    "    txt = re.sub(r\"[^\\w가-힣\\s]\", \" \", txt)              # 특수문자\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt).strip()               # 중복 공백\n",
    "    return txt.lower()\n",
    "\n",
    "df[\"clean\"] = df[\"text\"].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SimCSE 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name BM-K/KoSimCSE-roberta. Creating a new one with mean pooling.\n",
      "100%|██████████| 1/1 [00:03<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_ID = \"BM-K/KoSimCSE-roberta\"      # unsupervised 버전\n",
    "embedder = SentenceTransformer(MODEL_ID)\n",
    "\n",
    "BATCH = 256\n",
    "emb_list = []\n",
    "for i in tqdm(range(0, len(df), BATCH)):\n",
    "    batch = df[\"clean\"].iloc[i : i + BATCH].tolist()\n",
    "    embs  = embedder.encode(batch, batch_size=len(batch),\n",
    "                            normalize_embeddings=True)\n",
    "    emb_list.extend(embs)\n",
    "import numpy as np\n",
    "emb = np.vstack(emb_list).astype(\"float32\")              # (N, 768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalize_embeddings=True → 이미 L2 노멀라이즈된 벡터라 Inner Product = 코사인."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. FAISS 인덱스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "d = emb.shape[1]                     # 768\n",
    "index = faiss.IndexFlatIP(d)         # 작은 데이터셋은 Flat IP로 충분\n",
    "index.add(emb)                       # 전체 아이디어 삽입"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 유사 아이디어 검색 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.68017864), (7, 0.46029842), (8, 0.41422057), (1, 0.40591317), (9, 0.40527844)]\n"
     ]
    }
   ],
   "source": [
    "def find_similar(query: str, top_k: int = 5):\n",
    "    q_emb = embedder.encode([clean(query)],\n",
    "                            normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = index.search(q_emb, top_k)      # D: 코사인, I: 행 인덱스\n",
    "    return list(zip(I[0], D[0]))           # [(idx, score), …]\n",
    "\n",
    "print(find_similar(\"로봇 바리스타 카페 창업 아이디어\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. HDBSCAN 클러스터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster\n",
      " 1    5\n",
      "-1    3\n",
      " 0    2\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hdbscan.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hdbscan, joblib, numpy as np\n",
    "\n",
    "n = len(df)                                     # 현재 데이터 크기\n",
    "min_cluster = max(2, int(0.2 * n))              # 20% 또는 최소 2\n",
    "min_samples = min(min_cluster, n - 1)\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "        metric=\"euclidean\",\n",
    "        min_cluster_size=min_cluster,\n",
    "        min_samples=min_samples,\n",
    "        prediction_data=True\n",
    ").fit(emb)\n",
    "\n",
    "df[\"cluster\"] = clusterer.labels_\n",
    "print(df[\"cluster\"].value_counts())\n",
    "joblib.dump(clusterer, \"hdbscan.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기부터는 .py 에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 새 글 추가 → 실시간 검색 + 인덱스 갱신"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_idea(new_row: dict, search_k: int = 5):\n",
    "    \"\"\"\n",
    "    새 아이디어 1건을\n",
    "      1) 전처리·임베딩\n",
    "      2) FAISS 검색 → 유사도 top-k 반환\n",
    "      3) 인덱스·데이터프레임·임베딩 배열 업데이트\n",
    "    \"\"\"\n",
    "    global emb, df, index\n",
    "\n",
    "    # 1) 전처리 + 임베딩\n",
    "    cleaned = clean(new_row[\"title\"] + \" \" + new_row[\"body\"])\n",
    "    vec     = embedder.encode([cleaned], normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "    # 2) 유사도 검색\n",
    "    D, I = index.search(vec, search_k)\n",
    "    similar = [(int(idx), float(score)) for idx, score in zip(I[0], D[0])]\n",
    "\n",
    "    # 3-A) 인덱스·임베딩 배열 업데이트\n",
    "    index.add(vec)                          # FAISS에 즉시 반영\n",
    "    emb = np.vstack([emb, vec])             # ndarray 확장\n",
    "\n",
    "    # 3-B) 데이터프레임 업데이트\n",
    "    #     ★ 여기서 오류가 났던 부분 → 중괄호 개수 수정\n",
    "    df = pd.concat(\n",
    "        [df, pd.DataFrame([ new_row | {\"clean\": cleaned} ])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    return similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: HDBSCAN은 증분 학습이 불가하므로\n",
    "cron or Airflow로 5분마다 전체 재빌드 (clusterer.fit(emb))를 돌리면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 프론트 연동 초간단 API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.115.14-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 (from fastapi)\n",
      "  Using cached pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from fastapi) (4.14.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting anyio<5,>=3.6.2 (from starlette<0.47.0,>=0.40.0->fastapi)\n",
      "  Using cached anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/leeyoungho/miniforge3/envs/ai_3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
      "Collecting sniffio>=1.1 (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Downloading fastapi-0.115.14-py3-none-any.whl (95 kB)\n",
      "Using cached pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: typing-inspection, sniffio, pydantic-core, annotated-types, pydantic, anyio, starlette, fastapi\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [fastapi]m5/8\u001b[0m [anyio]ic]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.9.0 fastapi-0.115.14 pydantic-2.11.7 pydantic-core-2.33.2 sniffio-1.3.1 starlette-0.46.2 typing-inspection-0.4.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{str(sys.executable)} -m pip install fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/submit\")\n",
    "def submit(idea: dict):\n",
    "    sim = add_idea(idea)\n",
    "    # 코사인 0.7↑ + 같은 cluster 아이템만 추천\n",
    "    recs = [idx for idx, sc in sim if sc > 0.7 and\n",
    "            df.loc[idx, \"cluster\"] == df.iloc[-1][\"cluster\"]]\n",
    "    return {\"similar_ids\": recs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: pydantic[dotenv]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{str(sys.executable)} -m pip install fastapi uvicorn pydantic[dotenv] requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
