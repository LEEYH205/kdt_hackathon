import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
import pickle
import os
from typing import List, Dict, Tuple
import re

class PolicyChatbot:
    def __init__(self, csv_path: str = "bizinfo.csv", model_name: str = "sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens"):
        """
        ì •ì±… ì±—ë´‡ ì´ˆê¸°í™”
        
        Args:
            csv_path: ì •ì±… ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ
            model_name: ì„ë² ë”© ëª¨ë¸ëª…
        """
        self.csv_path = csv_path
        self.model_name = model_name
        self.data = None
        self.embeddings = None
        self.index = None
        self.model = None
        
        # ë°ì´í„° ë¡œë“œ ë° ëª¨ë¸ ì´ˆê¸°í™”
        self._load_data()
        self._initialize_model()
        self._create_embeddings()
        
    def _load_data(self):
        """CSV ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        try:
            self.data = pd.read_csv(self.csv_path)
            print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.data)}ê°œ ì •ì±…")
            
            # ê²°ì¸¡ê°’ ì²˜ë¦¬
            self.data = self.data.fillna("")
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            self.data['processed_text'] = self.data.apply(self._preprocess_text, axis=1)
            
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _preprocess_text(self, row: pd.Series) -> str:
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ì£¼ìš” í•„ë“œë“¤ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸ ìƒì„±
        fields = [
            str(row['title(ê³µê³ ëª…)']),
            str(row['body text (ê³µê³ ë‚´ìš©)']),
            str(row['ì§€ì›ëŒ€ìƒ']),
            str(row['ì†Œê´€ê¸°ê´€']),
            str(row['ì§€ì›ë¶„ì•¼(ëŒ€)']),
            str(row['ì§€ì›ë¶„ì•¼(ì¤‘)']),
            str(row['ì‚¬ì—…ìˆ˜í–‰ê¸°ê´€']),
            str(row['ë¬¸ì˜ì²˜']),
            str(row['ì‹ ì²­ê¸°ê°„']),
            str(row['ì‚¬ì—…ì‹ ì²­ë°©ë²•ì„¤ëª…'])
        ]
        
        # í…ìŠ¤íŠ¸ ê²°í•© ë° ì •ë¦¬
        combined_text = " ".join(fields)
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°± ì •ë¦¬
        combined_text = re.sub(r'[^\w\sê°€-í£]', ' ', combined_text)
        combined_text = re.sub(r'\s+', ' ', combined_text).strip()
        
        return combined_text
    
    def _initialize_model(self):
        """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            print("ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
            self.model = SentenceTransformer(self.model_name)
            print("ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            # í•œêµ­ì–´ì— íŠ¹í™”ëœ ëª¨ë¸ë¡œ ëŒ€ì²´
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
    
    def _create_embeddings(self):
        """í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± ë° FAISS ì¸ë±ìŠ¤ êµ¬ì¶•"""
        try:
            print("ì„ë² ë”© ìƒì„± ì¤‘...")
            
            # í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
            texts = self.data['processed_text'].tolist()
            self.embeddings = self.model.encode(texts, show_progress_bar=True)
            
            # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
            dimension = self.embeddings.shape[1]
            self.index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity)
            
            # ì •ê·œí™” (cosine similarityë¥¼ ìœ„í•´)
            faiss.normalize_L2(self.embeddings)
            self.index.add(self.embeddings.astype('float32'))
            
            print(f"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(self.embeddings)}ê°œ ë²¡í„°")
            
        except Exception as e:
            print(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def search_policies(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        ì •ì±… ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            processed_query = re.sub(r'[^\w\sê°€-í£]', ' ', query)
            processed_query = re.sub(r'\s+', ' ', processed_query).strip()
            
            # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
            query_embedding = self.model.encode([processed_query])
            faiss.normalize_L2(query_embedding)
            
            # ìœ ì‚¬ë„ ê²€ìƒ‰
            scores, indices = self.index.search(query_embedding.astype('float32'), top_k)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            results = []
            for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
                if idx < len(self.data):
                    row = self.data.iloc[idx]
                    result = {
                        'rank': i + 1,
                        'similarity_score': float(score),
                        'title': row['title(ê³µê³ ëª…)'],
                        'body': row['body text (ê³µê³ ë‚´ìš©)'],
                        'target': row['ì§€ì›ëŒ€ìƒ'],
                        'organization': row['ì†Œê´€ê¸°ê´€'],
                        'field_major': row['ì§€ì›ë¶„ì•¼(ëŒ€)'],
                        'field_minor': row['ì§€ì›ë¶„ì•¼(ì¤‘)'],
                        'executing_org': row['ì‚¬ì—…ìˆ˜í–‰ê¸°ê´€'],
                        'contact': row['ë¬¸ì˜ì²˜'],
                        'period': row['ì‹ ì²­ê¸°ê°„'],
                        'application_method': row['ì‚¬ì—…ì‹ ì²­ë°©ë²•ì„¤ëª…']
                    }
                    results.append(result)
            
            return results
            
        except Exception as e:
            print(f"ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_policy_summary(self, query: str) -> str:
        """ì •ì±… ìš”ì•½ ì •ë³´ ìƒì„±"""
        results = self.search_policies(query, top_k=3)
        
        if not results:
            return "ê´€ë ¨ ì •ì±…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        summary = f"'{query}'ì™€ ê´€ë ¨ëœ ì •ì±…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n"
        
        for result in results:
            summary += f"ğŸ“‹ {result['title']}\n"
            summary += f"ğŸ¯ ì§€ì›ëŒ€ìƒ: {result['target']}\n"
            summary += f"ğŸ¢ ì†Œê´€ê¸°ê´€: {result['organization']}\n"
            summary += f"ğŸ“… ì‹ ì²­ê¸°ê°„: {result['period']}\n"
            summary += f"ğŸ“ ë¬¸ì˜ì²˜: {result['contact']}\n"
            summary += f"ğŸ“ ì‹ ì²­ë°©ë²•: {result['application_method'][:100]}...\n"
            summary += f"ğŸ“Š ìœ ì‚¬ë„ ì ìˆ˜: {result['similarity_score']:.3f}\n"
            summary += "-" * 50 + "\n"
        
        return summary
    
    def save_model(self, path: str = "policy_chatbot_model.pkl"):
        """ëª¨ë¸ ì €ì¥"""
        try:
            model_data = {
                'data': self.data,
                'embeddings': self.embeddings,
                'index': self.index,
                'model_name': self.model_name
            }
            
            with open(path, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {path}")
            
        except Exception as e:
            print(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_model(self, path: str = "policy_chatbot_model.pkl"):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            with open(path, 'rb') as f:
                model_data = pickle.load(f)
            
            self.data = model_data['data']
            self.embeddings = model_data['embeddings']
            self.index = model_data['index']
            self.model_name = model_data['model_name']
            
            # ëª¨ë¸ ì¬ì´ˆê¸°í™”
            self._initialize_model()
            
            print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {path}")
            
        except Exception as e:
            print(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì±—ë´‡ ì´ˆê¸°í™”
    chatbot = PolicyChatbot()
    
    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
    test_queries = [
        "ì¤‘ì†Œê¸°ì—… ê¸°ìˆ ì§€ì›",
        "ì°½ì—… ì§€ì›",
        "ìˆ˜ì¶œ ì§„ì¶œ",
        "ì²­ë…„ ì§€ì›",
        "AI ê¸°ìˆ  ê°œë°œ"
    ]
    
    for query in test_queries:
        print(f"\nğŸ” ê²€ìƒ‰ì–´: {query}")
        print("=" * 50)
        results = chatbot.search_policies(query, top_k=3)
        
        for result in results:
            print(f"ğŸ“‹ {result['title']}")
            print(f"ğŸ¯ {result['target']} | ğŸ“Š ìœ ì‚¬ë„: {result['similarity_score']:.3f}")
            print("-" * 30) 